{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "INITIAL_LEARNING_RATE = 1e-3\n",
    "DECAY_STEPS = 2000\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.9  # The learning rate decay factor\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "REPORT_STEPS = 5000\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "TRAIN_SIZE = 7368\n",
    "BATCHES = TRAIN_SIZE//BATCH_SIZE\n",
    "test_num = 3\n",
    "\n",
    "ti = 'test_train'         \n",
    "vi = 'temp'        \n",
    "img_size = [94, 24]\n",
    "tl = None\n",
    "vl = None\n",
    "num_channels = 3\n",
    "label_len = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = [\n",
    "         '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "         'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K',\n",
    "         'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
    "         'W', 'X', 'Y', 'Z', '_'\n",
    "         ]\n",
    "\n",
    "dict = {}\n",
    "\n",
    "CHARS_DICT = {char:i for i, char in enumerate(CHARS)}\n",
    "\n",
    "NUM_CHARS = len(CHARS)\n",
    "\n",
    "\n",
    "def encode_label(s):\n",
    "    label = np.zeros([len(s)])\n",
    "    for i, c in enumerate(s):\n",
    "        label[i] = CHARS_DICT[c]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextImageGenerator:\n",
    "    def __init__(self, img_dir, label_file, batch_size, img_size, num_channels=3, label_len=7):\n",
    "        self._img_dir = img_dir\n",
    "        self._label_file = label_file\n",
    "        self._batch_size = batch_size\n",
    "        self._num_channels = num_channels\n",
    "        self._label_len = label_len\n",
    "        self._img_w, self._img_h = img_size\n",
    "\n",
    "        self._num_examples = 0\n",
    "        self._next_index = 0\n",
    "        self._num_epoches = 0\n",
    "        self.filenames = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "        self.labels = []\n",
    "        fs = os.listdir(self._img_dir)\n",
    "        for filename in fs:\n",
    "                self.filenames.append(filename)\n",
    "        for filename in self.filenames:\n",
    "            # if len(filename) > 15:\n",
    "            #     label = dict[filename[:3]] + filename[4:10]\n",
    "            # else:\n",
    "            if \"_\" not in filename:\n",
    "                label = filename.split(\".\")[0]\n",
    "            else:\n",
    "                label = filename.split(\"_\")[0]\n",
    "            label = encode_label(label)\n",
    "            self.labels.append(label)\n",
    "            self._num_examples += 1\n",
    "\n",
    "    def next_batch(self):\n",
    "        # Shuffle the data\n",
    "        if self._next_index == 0:\n",
    "            perm = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self._filenames = [self.filenames[i] for i in perm]\n",
    "            self._labels = [self.labels[i] for i in perm]\n",
    "\n",
    "        batch_size = self._batch_size\n",
    "        start = self._next_index\n",
    "        end = self._next_index + batch_size\n",
    "        if end > self._num_examples:\n",
    "            self._next_index = 0\n",
    "            start = self._next_index\n",
    "            end = self._next_index + batch_size\n",
    "            self._num_epoches += 1\n",
    "        else:\n",
    "            self._next_index = end\n",
    "        images = np.zeros([batch_size, self._img_h, self._img_w, self._num_channels])\n",
    "        # labels = np.zeros([batch_size, self._label_len])\n",
    "\n",
    "        for j, i in enumerate(range(start, end)):\n",
    "            fname = self._filenames[i]\n",
    "            img = cv2.imread(os.path.join(self._img_dir, fname))\n",
    "            img = cv2.resize(img, (self._img_w, self._img_h), interpolation=cv2.INTER_CUBIC)\n",
    "            images[j, ...] = img\n",
    "        images = np.transpose(images, axes=[0, 2, 1, 3])\n",
    "        labels = self._labels[start:end]\n",
    "        targets = [np.asarray(i) for i in labels]\n",
    "        sparse_labels = sparse_tuple_from(targets)\n",
    "        # input_length = np.zeros([batch_size, 1])\n",
    "\n",
    "        seq_len = np.ones(self._batch_size) * 24\n",
    "        return images, sparse_labels, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"\n",
    "    Create a sparse representention of x.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n] * len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape\n",
    "\n",
    "\n",
    "def decode_sparse_tensor(sparse_tensor):\n",
    "    decoded_indexes = list()\n",
    "    current_i = 0\n",
    "    current_seq = []\n",
    "    for offset, i_and_index in enumerate(sparse_tensor[0]):\n",
    "        i = i_and_index[0]\n",
    "        if i != current_i:\n",
    "            decoded_indexes.append(current_seq)\n",
    "            current_i = i\n",
    "            current_seq = list()\n",
    "        current_seq.append(offset)\n",
    "    decoded_indexes.append(current_seq)\n",
    "    result = []\n",
    "    for index in decoded_indexes:\n",
    "        result.append(decode_a_seq(index, sparse_tensor))\n",
    "    return result\n",
    "\n",
    "\n",
    "def decode_a_seq(indexes, spars_tensor):\n",
    "    decoded = []\n",
    "    for m in indexes:\n",
    "        str = CHARS[spars_tensor[1][m]]\n",
    "        decoded.append(str)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_basic_block(x, im, om):\n",
    "    x = conv(x,im,int(om/4),ksize=[1,1])\n",
    "    x = conv(x,int(om/4),int(om/4),ksize=[3,1],pad='SAME')\n",
    "    x = conv(x,int(om/4),int(om/4),ksize=[1,3],pad='SAME')\n",
    "    x = conv(x,int(om/4),om,ksize=[1,1])\n",
    "    return x\n",
    "\n",
    "def conv(x,im,om,ksize,stride=[1,1,1,1],pad = 'SAME'):\n",
    "    conv_weights = tf.Variable(\n",
    "        tf.truncated_normal([ksize[0], ksize[1], im, om],  # 5x5 filter, depth 32.\n",
    "                            stddev=0.1,\n",
    "                            seed=None, dtype=tf.float32))\n",
    "    conv_biases = tf.Variable(tf.zeros([om], dtype=tf.float32))\n",
    "    out = tf.nn.conv2d(x,\n",
    "                        conv_weights,\n",
    "                        strides=stride,\n",
    "                        padding=pad)\n",
    "    relu = tf.nn.bias_add(out, conv_biases)\n",
    "    return relu\n",
    "\n",
    "def get_train_model(num_channels, label_len, b, img_size):\n",
    "    inputs = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=(b, img_size[0], img_size[1], num_channels))\n",
    "\n",
    "    # 定义ctc_loss需要的稀疏矩阵\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "\n",
    "    # 1维向量 序列长度 [batch_size,]\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    x = inputs\n",
    "\n",
    "    x = conv(x,num_channels,64,ksize=[3,3])\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.nn.max_pool(x,\n",
    "                          ksize=[1, 3, 3, 1],\n",
    "                          strides=[1, 1, 1, 1],\n",
    "                          padding='SAME')\n",
    "    x = small_basic_block(x,64,64)\n",
    "    x2=x\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = tf.nn.max_pool(x,\n",
    "                          ksize=[1, 3, 3, 1],\n",
    "                          strides=[1, 2, 1, 1],\n",
    "                          padding='SAME')\n",
    "    x = small_basic_block(x, 64,256)\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = small_basic_block(x, 256, 256)\n",
    "    x3 = x\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.nn.max_pool(x,\n",
    "                       ksize=[1, 3, 3, 1],\n",
    "                       strides=[1, 2, 1, 1],\n",
    "                       padding='SAME')\n",
    "    x = tf.layers.dropout(x)\n",
    "\n",
    "    x = conv(x, 256, 256, ksize=[4, 1])\n",
    "    x = tf.layers.dropout(x)\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "\n",
    "    x = conv(x,256,NUM_CHARS+1,ksize=[1,13],pad='SAME')\n",
    "    x = tf.nn.relu(x)\n",
    "    cx = tf.reduce_mean(tf.square(x))\n",
    "    x = tf.div(x,cx)\n",
    "\n",
    "    #x = tf.reduce_mean(x,axis = 2)\n",
    "    #x1 = conv(inputs,num_channels,num_channels,ksize = (5,1))\n",
    "\n",
    "\n",
    "    x1 = tf.nn.avg_pool(inputs,\n",
    "                       ksize=[1, 4, 1, 1],\n",
    "                       strides=[1, 4, 1, 1],\n",
    "                       padding='SAME')\n",
    "    cx1 = tf.reduce_mean(tf.square(x1))\n",
    "    x1 = tf.div(x1, cx1)\n",
    "\n",
    "    # x1 = tf.image.resize_images(x1, size = [18, 16], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    x2 = tf.nn.avg_pool(x2,\n",
    "                        ksize=[1, 4, 1, 1],\n",
    "                        strides=[1, 4, 1, 1],\n",
    "                        padding='SAME')\n",
    "    cx2 = tf.reduce_mean(tf.square(x2))\n",
    "    x2 = tf.div(x2, cx2)\n",
    "\n",
    "    #x2 = tf.image.resize_images(x2, size=[18, 16], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    x3 = tf.nn.avg_pool(x3,\n",
    "                        ksize=[1, 2, 1, 1],\n",
    "                        strides=[1, 2, 1, 1],\n",
    "                        padding='SAME')\n",
    "    cx3 = tf.reduce_mean(tf.square(x3))\n",
    "    x3 = tf.div(x3, cx3)\n",
    "\n",
    "    #x3 = tf.image.resize_images(x3, size=[18, 16], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "\n",
    "    #x1 = tf.nn.relu(x1)\n",
    "\n",
    "    x = tf.concat([x,x1,x2,x3],3)\n",
    "    x = conv(x, x.get_shape().as_list()[3], NUM_CHARS + 1, ksize=(1, 1))\n",
    "    logits = tf.reduce_mean(x,axis=2)\n",
    "    # x_shape = x.get_shape().as_list()\n",
    "    # outputs = tf.reshape(x, [-1,x_shape[2]*x_shape[3]])\n",
    "    # W1 = tf.Variable(tf.truncated_normal([x_shape[2]*x_shape[3],\n",
    "    #                                      150],\n",
    "    #                                     stddev=0.1))\n",
    "    # b1 = tf.Variable(tf.constant(0., shape=[150]))\n",
    "    # # [batch_size*max_timesteps,num_classes]\n",
    "    # x = tf.matmul(outputs, W1) + b1\n",
    "    # x= tf.layers.dropout(x)\n",
    "    # x = tf.nn.relu(x)\n",
    "    # W2 = tf.Variable(tf.truncated_normal([150,\n",
    "    #                                      NUM_CHARS+1],\n",
    "    #                                     stddev=0.1))\n",
    "    # b2 = tf.Variable(tf.constant(0., shape=[NUM_CHARS+1]))\n",
    "    # x = tf.matmul(x, W2) + b2\n",
    "    # x = tf.layers.dropout(x)\n",
    "    # # [batch_size,max_timesteps,num_classes]\n",
    "    # logits = tf.reshape(x, [b, -1, NUM_CHARS+1])\n",
    "\n",
    "    return logits, inputs, targets, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(a):\n",
    "\n",
    "    train_gen = TextImageGenerator(img_dir=ti,\n",
    "                                   label_file=tl,\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   img_size=img_size,\n",
    "                                   num_channels=num_channels,\n",
    "                                   label_len=label_len)\n",
    "\n",
    "    val_gen = TextImageGenerator(img_dir=vi,\n",
    "                                 label_file=vl,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 img_size=img_size,\n",
    "                                 num_channels=num_channels,\n",
    "                                 label_len=label_len)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                               global_step,\n",
    "                                               DECAY_STEPS,\n",
    "                                               LEARNING_RATE_DECAY_FACTOR,\n",
    "                                               staircase=True)\n",
    "    logits, inputs, targets, seq_len = get_train_model(num_channels, label_len,BATCH_SIZE, img_size)\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "    # tragets是一个稀疏矩阵\n",
    "    loss = tf.nn.ctc_loss(labels=targets, inputs=logits, sequence_length=seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=MOMENTUM).minimize(cost, global_step=global_step)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # 前面说的划分块之后找每块的类属概率分布，ctc_beam_search_decoder方法,是每次找最大的K个概率分布\n",
    "    # 还有一种贪心策略是只找概率最大那个，也就是K=1的情况ctc_ greedy_decoder\n",
    "    decoded, log_prob = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "\n",
    "    acc = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), targets))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    def report_accuracy(decoded_list, test_targets):\n",
    "        original_list = decode_sparse_tensor(test_targets)\n",
    "        detected_list = decode_sparse_tensor(decoded_list)\n",
    "        true_numer = 0\n",
    "\n",
    "        if len(original_list) != len(detected_list):\n",
    "            print(\"len(original_list)\", len(original_list), \"len(detected_list)\", len(detected_list),\n",
    "                  \" test and detect length desn't match\")\n",
    "            return\n",
    "        print(\"T/F: original(length) <-------> detectcted(length)\")\n",
    "        for idx, number in enumerate(original_list):\n",
    "            detect_number = detected_list[idx]\n",
    "            hit = (number == detect_number)\n",
    "            print(hit, number, \"(\", len(number), \") <-------> \", detect_number, \"(\", len(detect_number), \")\")\n",
    "            if hit:\n",
    "                true_numer = true_numer + 1\n",
    "        print(\"Test Accuracy:\", true_numer * 1.0 / len(original_list))\n",
    "\n",
    "    def do_report(val_gen,num):\n",
    "        for i in range(num):\n",
    "            test_inputs, test_targets, test_seq_len = val_gen.next_batch()\n",
    "            test_feed = {inputs: test_inputs,\n",
    "                        targets: test_targets,\n",
    "                        seq_len: test_seq_len}\n",
    "            st =time.time()\n",
    "            dd= session.run(decoded[0], test_feed)\n",
    "            tim = time.time() -st\n",
    "            print('time:%s'%tim)\n",
    "            report_accuracy(dd, test_targets)\n",
    "\n",
    "    def test_report(testi,files):\n",
    "        true_numer = 0\n",
    "        num = files//BATCH_SIZE\n",
    "\n",
    "        for i in range(num):\n",
    "            test_inputs, test_targets, test_seq_len = val_gen.next_batch()\n",
    "            test_feed = {inputs: test_inputs,\n",
    "                        targets: test_targets,\n",
    "                        seq_len: test_seq_len}\n",
    "            dd = session.run([decoded[0]], test_feed)\n",
    "            original_list = decode_sparse_tensor(test_targets)\n",
    "            detected_list = decode_sparse_tensor(dd)\n",
    "            for idx, number in enumerate(original_list):\n",
    "                detect_number = detected_list[idx]\n",
    "                hit = (number == detect_number)\n",
    "                if hit:\n",
    "                    true_numer = true_numer + 1\n",
    "        print(\"Test Accuracy:\", true_numer * 1.0 / files)\n",
    "\n",
    "\n",
    "    def do_batch(train_gen,val_gen):\n",
    "        train_inputs, train_targets, train_seq_len = train_gen.next_batch()\n",
    "\n",
    "        feed = {inputs: train_inputs, targets: train_targets, seq_len: train_seq_len}\n",
    "\n",
    "        b_loss, b_targets, b_logits, b_seq_len, b_cost, steps, _ = session.run(\n",
    "            [loss, targets, logits, seq_len, cost, global_step, optimizer], feed)\n",
    "\n",
    "        #print(b_cost, steps)\n",
    "        if steps > 0 and steps % REPORT_STEPS == 0:\n",
    "            do_report(val_gen,test_num)\n",
    "            saver.save(session, \"./model/LPRtf3.ckpt\", global_step=steps)\n",
    "        return b_cost, steps\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=100)\n",
    "        if a=='train':\n",
    "            for curr_epoch in range(num_epochs):\n",
    "                print(\"Epoch.......\", curr_epoch)\n",
    "                train_cost = train_ler = 0\n",
    "                for batch in range(BATCHES):\n",
    "                    start = time.time()\n",
    "                    c, steps = do_batch(train_gen,val_gen)\n",
    "                    train_cost += c * BATCH_SIZE\n",
    "                    seconds = time.time() - start\n",
    "                    #print(\"Step:\", steps, \", batch seconds:\", seconds)\n",
    "\n",
    "                train_cost /= TRAIN_SIZE\n",
    "                val_cs=0\n",
    "                val_ls =0\n",
    "                for i in range(test_num):\n",
    "                    train_inputs, train_targets, train_seq_len = val_gen.next_batch()\n",
    "                    val_feed = {inputs: train_inputs,\n",
    "                                targets: train_targets,\n",
    "                                seq_len: train_seq_len}\n",
    "\n",
    "                    val_cost, val_ler, lr, steps = session.run([cost, acc, learning_rate, global_step], feed_dict=val_feed)\n",
    "                    val_cs+=val_cost\n",
    "                    val_ls+=val_ler\n",
    "\n",
    "                log = \"Epoch {}/{}, steps = {}, train_cost = {:.3f}, train_ler = {:.3f}, val_cost = {:.3f}, val_ler = {:.3f}, time = {:.3f}s, learning_rate = {}\"\n",
    "                print(log.format(curr_epoch + 1, num_epochs, steps, train_cost, train_ler, val_cs/test_num, val_ls/test_num,\n",
    "                                 time.time() - start, lr))\n",
    "        if a =='test':\n",
    "            testi='temp'\n",
    "            saver.restore(session, './model8.24best/LPRtf3.ckpt-25000')\n",
    "            test_gen = TextImageGenerator(img_dir=testi,\n",
    "                                           label_file=None,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           img_size=img_size,\n",
    "                                           num_channels=num_channels,\n",
    "                                           label_len=label_len)\n",
    "            do_report(test_gen, 3)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        a = input('train or test:')\n",
    "        train(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
